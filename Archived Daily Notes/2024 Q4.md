


19/12/24
 - testing binary
 - infra for GHA
 - 

16/12/24
- Integration test
- Rob FASTQ upload
- Testing changes on Friday - failed as ubuntu HPC script needs updating
17/12/24
- Fix ubuntu HCP script (resize VM)
- integration testing
- Etienne catch up
- 

12/12/24
- factory
	- controller > service > repo
- pass pointer to config everywhere - so it can be edited
	- as opposed to an object

09/12/24
- Christ almighty
- europe-docker.pkg.dev-cegx-releases-eu-prod-fastqc-0.2.0.img - what it needs to be
- what it is: 
- -rwxr-xr-x. 1 nathan_admin_cegx_co_uk nathan_admin_cegx_co_uk 155455488 **Dec  9 19:07** europe-docker.pkg.dev-cegx-releases-eu-prod-pipelinereport-0-3-4.img
-rwxr-xr-x. 1 nathan_admin_cegx_co_uk nathan_admin_cegx_co_uk 155455488 **Dec  9 19:38** europe-docker.pkg.dev-cegx-releases-eu-prod-pipelinereport-0.3.4.img

[@EBoisseauSierra](https://github.com/EBoisseauSierra) whilst my other draft PR works, it was becoming messy/ built on mess. I also realised that the download service needs refactoring to incorporate HPC, so `biomodal init` fails. FYI [@khaug](https://github.com/khaug) .

What do you think of the organisation here? My reasoning, is that not a huge amount is shared between these platform groups. Think this concern also extends to the download service, as it's written, push is an intrinsic part of it.

We could have a top-level, with shared functions:

### L1: repositories/container_repository

- Get source container repository (biomodal_repository)
- GetPlatform
- GetImageVersions

### L2 repositories/container_repository/docker_container_repository

- To make it DRY, put the auth into the auth repo and call from the respective pull repository
- interface
- docker_pull_repository
- gcp_push_repository/ gcp_client.go (these will contain client + push command)
- aws_push_repository
- azure_push_repository

### L2 repositories/container_repository/hpc_container_repository

- To make it DRY, put the auth into the auth repo and call from the respective pull repository
- interface
- hpc_container_repository
- hpc_pull_container_repository

I think what I'm suggesting here is nested packages, not sure if this is anti-pattern for Go however. My currently perceived benefit of this is that both nested packages don't (shouldn't?) need to be initialised at program run time, we can initialise the respective package based on platform value.

Side note: I have functions like GetPlatform & GetSourceRepo in the container_repository, should I not just call the service, as this is effectively a reimplementation? The container repositories should be focused on actions regarding the relevant source and destination repositories.

05/12/24: At the infra catch-up, we discussed initialisation order. It seems to make more sense that we initialise command specific services+repos in the controller. This should reduce coupling & unnecessary dependencies in `main.go`. Some things like config service should probably continue to be run regardless of command, as their intrinsic to all operations, we could also argue auth is part of more commands than it is not (just test + analyse?) and should be initialised everytime.

This may yet need another refactor when we come to doing AWS + Azure, as we don't know if the docker golang sdk will play nicely with them, and could need to revert to cloud provider SDK's to upload images. If this were the case, presumable we will need separate 'push' methods for each. However, if they will work with the standard docker go sdk, we should just be able to have a CloudClient interface, implemented in _client.go, with a client_factory.go to initialise the correct client. Push can then be called with something like this: `docker_container_repository.NewCloudClient(platform)`.

Whether we're able to use cloud auth + docker sdk, or have to use cloud auth + provider container sdk, we should be able to keep DRY this way.

Assuming the above will be a common factor with this CLI (business logic > platform??? > action), are we happy to embrace factories in the repositories?

02/12/24
- 

25/11/24
- apptainer pull
```
SINGULARITY_DOCKER_USERNAME=oauth2accesstoken \
SINGULARITY_DOCKER_PASSWORD="${docker_access_token}" \
singularity pull \
  "${bucket_url}/singularity-images/$(nxf_singularity_image_name "${biomodal_registry}/${img}")" \
  "docker://${biomodal_registry}/${img}"

bucket_url="/bimoodal/data_bucket"
biomodal_registry="europe-docker.pkg.dev/cegx-releases/eu-prod"
img - list of images from released versions config


nxf_singularity_image_name() {
local docker_image_url; docker_image_url="${1}"
echo "${docker_image_url}.img" | tr '/:' '-'
}

```




19/11/24
- Azure - 11/11 1hr
- AZ 5/11 2hr
- AZ 28/10 6hr
- AZ 14/10 2hr
- AZ 9/10 2hr



18/11/2024
- apptainer/singularity investigation summary:
	- Nextflow needs singularity or apptainer - therefore we have to assume one of these dependencies is present
	- If we have to assume this, then we can rely on one being used to run the pull command
		- Problem there is getting auth to work, however we already make a token in the client, so should be fine? Needs to be tested anyway
	- Looks like [[GetPackage/cloud.google.com/go/artifactregistry/apiv1#example-Client.GetPackage]]  might work? Worth testing, then seeing if 

13/11/24
- Fixing hpc admin boot
- Test conda boot
- singularity remote login
- gatk & python3 repos ci/cd

12/11/24
- singularity remote login
- Fixing hpc admin boot

2 interfaces/ push+pull
- platform agnostic

11/11/24
- Review code
- Fix bugs - specifically the config errors are back
- Also panics
- Keep 
```
panic: failed to tag container asm:0.2.9 as europe-west2-docker.pkg.dev/cli-testing-404/nw-nextflow-vm/asm:0.2.9: Error response from daemon: No such image: asm:0.2.9
```


08/11/24
- Debating best path forward withrepos
- Create local testing instances

07/11/24
- Finish vuln PR
- HPC containers go cli

06/11/24
- Add vuln stuff to template repo
- Clean-up docker auth and fix clients

04/11/24
- docker auth PR tidy
- 

31/10/24
- Re-org.
	- New epics
- Docker auth:
	- We start with the IDtoken
	- This then needs to be changed to an access token
	- Which then needs to be used by the impersonation service?
	- Which then gets a token, used by docker client
- Start by printing the existing steps, outputs and order

30/10/24
- Artifact Reg auth
	- Test with clientconfig.json file with OAuth2
	- Would work with SA impersonate?


29/10/24
- Docker auth
- modality release process
- Adding:
```bash
gcloud artifacts repositories add-iam-policy-binding eu-prod \
    --project=cegx-releases \
    --location=europe \
    --member="principal://iam.googleapis.com/projects/1014946990955/locations/global/workloadIdentityPools/biomodal-cli-customers" \
    --role="roles/artifactregistry.reader"
```


28/10/24

Modality release issue:
1. We want to create a release, what happens now?
2. Run the create_release.yaml action?
	1. This creates a release branch that we assume can be incremented continuously until ready to merge
	2. What trigger is used to run the build and tag then? 
	3. If tag the codebase, when should that be run?


23/10/24
- add workflow call for python build
- binary-nextflow-modules
	- Collate all other docker only modules
- Ruff supports bandit, add this to the template repo: template_nextflow_module
	- Ignore only S01 (assert)
- Test on prelude add to pre-commit
- Scrap separate action - put into build_and_tag.yml
- REPO - look at miro pipeline interactions

22/10/24
- prelude GHA
- docker auth fix
21/10/24
- auth issue
- prelude sbom + grype
- x-plat tests pass






16/10/24
- Review path to feature parity
- docker auth

15/01/24
- cross plat
- docker auth
- Ci/CD doc review
- 

14/10/24
- doit request
- appointment
- Trying to copy files to azure
- docker auth
- 

11/10/24


Hi DoIT Team,

We're currently looking at reducing our GCS spend further, by archiving data last modified either at 1yr+ or 6mo+, however we have some concerns about potentially spending lots on archived data retrieval:
- Via the DoIT console (or other means you're aware of), can we track data consumption (either class A or B operations) on archived data easily?
- Currently, bucket access is given at project level, ideally we would remove access from archived buckets, do you have any ideas
- It is my understanding, that moving buckets between projects would incur no cost, provided they're in the same (single) region, is this correct?
- Would this have any implication for using the DoIT console, does DoIT scan buckets to get relevant data, thus resulting in a higher retrieval fee for archived buckets/ objects?

**Input needed - Review and tighten existing storage controls (Q4)**  
**Intro:**  
- As part of this Q4 initiative, we have identified a significant cost saving.  
- Presently a significant portion of our data in `cegx-nextflow` is in standard storage (the most expensive).
- If we change the storage class of all data over a year old (by last modified) to archive, we can make a monthly saving of $9990.
- If we go for an even more aggressive policy of any data older than 6 months (by last modified date), we can save $15808.50/month.
- Note: archived data **is not deleted**, nor does it take time to move back to standard, there is just a charge for bringing data out of of archive and back to standard OR reading/ copying/ moving data directly from archive (data does not have to be moved from archive to standard to be used, it is just costly to use archive data). 
**The questions:**  
- Are you aware of any data, either a subset in a bucket, or an entire bucket, that would fall into this category, which may need to be used again soon?  
- Is it a frequent occurrence that old data (+1year/ or +6months) is re-run?  
- Reason for asking is, there is a charge to move data out of archive once. Any action that reads an object in a bucket will move the it from archive to standard (pipeline runs, pulling to VM or local machine, anything other than `ls` really).Here is the full storage analysis:  

=== Overall Storage Analysis ===
Total Data: 1.08 PB
Data Over a Year Old: 455.75 TB
Data Over 6 Months Old: 753.07 TB
Current Cost (Standard Storage): $25989.93/month
Archive Cost (>1 Year Old): $16422.83/month
Potential Savings (>1 Year Old): $9567.10/month
Archive Cost (>6 Months Old): $10181.43/month
Potential Savings (>6 Months Old): $15808.50/month

I have attached the spreadsheet summarising the data, and a link to the script used to generate it is [here](https://github.com/cegx-ds/gcp-spend/blob/PE-2015/cegx-nextflow-archive/spend_tools/archive_cost_savings.py).  
Note: In the spreadsheet, some buckets have no data in Total Data, this is because archived objects were excluded. We would appreciate any feedback and if there are no major problems with the proposal we can put this in place towards the end of this week.



10/10/24
Cost saving quick wins:
- Runs older than 6 months move to ARCHIVE
- Delete ds-archive project ($1.7k pm)
- Delete cegx-test-project-n ($1.2k pm)
- combined costing ~3k pm


```

=== Overall Storage Analysis ===
Total Data: 1127.48 TB
Data Over a Year Old: 475.87 TB
Current Cost (Standard Storage): $26554.30/month
Archive Cost (After Moving Old Data): $16564.92/month
Potential Savings: $9989.38/month
Traceback (most recent call last):
  File "/home/nathan_admin_cegx_co_uk/get_cost.py", line 136, in <module>
    main()
  File "/home/nathan_admin_cegx_co_uk/get_cost.py", line 126, in main
    df = df.append(summary_row, ignore_index=True)
         ^^^^^^^^^
  File "/home/nathan_admin_cegx_co_uk/venv/lib/python3.11/site-packages/pandas/core/generic.py", line 6299, in __getattr__
    return object.__getattribute__(self, name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DataFrame' object has no attribute 'append'. Did you mean: '_append'
```

08/10/24
- Fixing the reference upload functio
- Fix file exists function
- Fix docker auth